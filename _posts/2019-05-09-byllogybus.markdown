---
title: "BYLLOGYBUS"
draft: true
layout: post
date: 2019-05-09 11:11
image: /assets/images/markdown.jpg
headerImage: false
tag:
- markdown
- elements
star: true
category: blog
author: maximonakpil
description: The essentials.
---

## BYLLOGYBUS
Syllabus in a Blog! Things to dig into to nourish insatiable developers.


---

## Philosophy and Logic
Cogito Ergo Sum - somehow explains the poetry of OOP with self instantiation.

## Cataclysmic Inventors
What was going on around them at at that time - From another cycle?
What are the intersections?


## Craft of Design
Taking ownership of the design - instantiation spin cycle. "Intuitive design" is the metronome
for when to drop a value to let the mechanical cogs enough momentum to turn. A good name or >3 character  
variable + combination of either defined or undefined object (or description) adds weight to the turning force.


## Roots and Etomologies

Theres is a lot of wisdom in the importance of how words were coined.
Naming convention is integral to good and intuitive design. Packaging information in a few letters.

The MASER vs LASER patent war is a juicy example. Discoveries are named after inventions to give meaning and reverence, in the same parent child pattern of root words and their derivations. The right word can be the right key to the needed mindset.

AD CE vs BC BCE as well is another highly debated academic war zone. Or how Claude Shannon coined Entropy with no solid proof of
valid relationship untill Gibbs Entropy paved way for Von Neumann entropy making a possible connnection with quantum theory.

Shannon's definition of entropy, when applied to an information source, can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits

https://en.wikipedia.org/wiki/Entropy_(information_theory)
https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem#Source_coding_theorem_for_symbol_codes

At an everyday practical level, the links between information entropy and thermodynamic entropy are not evident. Physicists and chemists are apt to be more interested in changes in entropy as a system spontaneously evolves away from its initial conditions, in accordance with the second law of thermodynamics, rather than an unchanging probability distribution. As the minuteness of Boltzmann's constant kB indicates, the changes in S / kB for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in data compression or signal processing. In classical thermodynamics, entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution, which is central to the definition of information entropy.

The connection between thermodynamics and what is now known as information theory was first made by Ludwig Boltzmann and expressed by his famous equation:

{\displaystyle S=k_{\text{B}}\ln(W)} {\displaystyle S=k_{\text{B}}\ln(W)}
where {\displaystyle S} S is the thermodynamic entropy of a particular macrostate (defined by thermodynamic parameters such as temperature, volume, energy, etc.), W is the number of microstates (various combinations of particles in various energy states) that can yield the given macrostate, and kB is Boltzmann's constant. It is assumed that each microstate is equally likely, so that the probability of a given microstate is pi = 1/W. When these probabilities are substituted into the above expression for the Gibbs entropy (or equivalently kB times the Shannon entropy), Boltzmann's equation results. In information theoretic terms, the information entropy of a system is the amount of "missing" information needed to determine a microstate, given the macrostate.

https://en.wikipedia.org/wiki/Maximum_entropy_thermodynamics
https://en.wikipedia.org/wiki/Huffman_coding
https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch
https://en.wikipedia.org/wiki/GIF

https://en.wikipedia.org/wiki/Checksum




Good words are the keys to receptivity that establishes the connections needed between well packaged concepts.

## Documentaries
Much like a well packaged concept, I will list out all the documentaries that have helped me improve my view.

Great Courses (amazon prime)
Information Technology
Computer Science Fundamentals with Python

Origins

## Future Blue Skies Predicting
Sci-fi isn't so silly after opening my horizons and attempting to fathom cosmology.
Im particularly invested in the red shift and doppler effect and how the according to a color pattern dictated by the cosmos, we can tell the time of the objects creation via its distance from us.

[Universe is Expanding][1]

So if the universe is expanding and then cooling, the temperature paints the picture of how far we are to the colored reference light..
[Cosmic Microwave][2] is the sanity check.

New discoveries help my imagination run wild and through out time, the art and humanities sciences documents the same collective inclination.

Extrapolating the scientific rules in the observable universe, essentially computer science was developed to simulate and calculate or aggragate data.


Star Trek somehow is a capsule of how our imagination worked during its peak as Black Mirror our modern scifi-pedia. It exposes the paradigm shifts and paradoxes that existed in the same brainwaves of the engineering minds. The design and continuity in a well thought out TV series or story creates important concept waves that exists to as far as we know.

Good information architecture, like the doppler effect, is to consider an idea and its distance from our current mindset. Dictating discoverable patterns for those who are willing to search.

Understanding what the union is between multiple sciences, also looking for the answers in the "inverse". The digital information age is like a minesweeper game with enough reading and discernment, I could clear out paths to solutions. The way we share has evolved like the internet epochs, now considering humanities evolving alongside technology. How our search usage evolved, with encylopedia britanica, wikipedia, google referntial search engines and now [dbpedia][3] and [union][4].

[1]: https://archive.briankoberlein.com/2013/09/11/hubbles-constant/index.html

[2]: https://archive.briankoberlein.com/2013/09/12/echo-of-the-big-bang/index.html

[3]: dbpedia

[4]: unionpedia

[5]: https://wiki.lesswrong.com/wiki/LessWrong_Wiki
